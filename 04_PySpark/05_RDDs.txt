#Starting the spark session in jupyter
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.stop()

import pyspark
from pyspark import SparkConf
from pyspark import SparkContext
#Creating a spark Context:
conf = SparkConf().setAppName('sparkRDD').setMaster('local')
sc = SparkContext(conf=conf)

#Creating RDD using list:
values = [2,5,9,7,5,6,5]
rdd = sc.parallelize(values)
#Printing first 5 values in RDD:
rdd.take(5)

============================================
RDD Persistence:

RDD Caching:




